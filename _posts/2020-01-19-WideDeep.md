---
layout: distill
title: "[추천시스템] Wide & Deep Learning for Recommender Systems"
date: '2021-01-20 12:37:28 -0400'
categories: 
  - 추천시스템
use_math: true
comments: true
toc: true
toc_sticky: true
toc_label: 목차
---

Cheng, Heng-Tze, et al. "Wide & deep learning for recommender systems." Proceedings of the 1st workshop on deep learning for recommender systems. 2016.


### _In short, "Memorization (Wide) + Generalization (Deep)"_


### About 

이 논문은 추천시스템의 Memorization, Generalization 의 장점을 결합한 새로운 학습 방법을 제시해주는 논문이다.

### Abstract

Generalization 모델은 적은 수의 인풋을 가진 대규모의 회귀나 분류 문제에 적합하고 Memorization 모델은 외적을 바탕으로 한 효과적이고 설명력이 높은 특성을 가진다.
이 논문에서는 Wide & Deep Learning 이라는 Wide Linear 모델과 DNN 의 장점만을 결합한 모델을 새롭게 제시한다.

### 1. Introduction

추천 시스템은 인풋으로 쿼리(데이터베이스에 정보를 요청하는)와 맥락적인 정보를 가지고 아웃풋으로는 순위가 매겨진 아이템을 가지는 랭킹 시스템이라고 볼 수 있다. 이러한 목적 하에 주로 발생하는 문제는 Memorization 과 Generalization 의 특성을 모두 가져가는 것이다. 간단히 설명하자면, **조류가 날아다니는 사진을 보여준 상황**에서 아래와 같이 설명할 수 있다.
- **Wide (Memorization)**: **비둘기는 날 수 있다**, **참새는 날 수 있다**
- **Deep (Generalization)**: **날개를 가진 동물은 날 수 있다** 

- **Wide & Deep**: **'날개를 가진 동물은 날 수 있지만, 펭귄은 날 수 없다'** 


<img src="https://user-images.githubusercontent.com/68312164/105043795-4d6e9e00-5aa9-11eb-805b-f0a9248b624e.png" width="200" height="250"> <img src="https://user-images.githubusercontent.com/68312164/105043929-798a1f00-5aa9-11eb-9b49-1962dc8b6210.png" width="200" height="250"> <img src="https://user-images.githubusercontent.com/68312164/105044095-ab9b8100-5aa9-11eb-9d07-523b7ea10fcc.png" width="200" height="250">

#### Wide Model
<img src="https://user-images.githubusercontent.com/68312164/105060951-4a30dd80-5abc-11eb-995c-214a73c3ddbd.png" width="600" height="300">

먼저 Wide 모델에서는 위와 같이 User feature, Impression feature 그리고 Crossed feature를 input으로 가지게 된다. 유저가 실제로 설치한 여행 앱, ’Priceline’ 을 1이라고 하고, 유저가 관심을 보인 ‘Kayak’ 앱을 1이라고 했을 때 둘을 곱한 1이 Crossed Feature가 된다. 이러한 방식은 1인 경우만 학습을 하기에 유저의 niche 한 특성이 잘 반영되는 한편, User feature, Impression feature 중 하나가 0 이면 Crossed feature 역시 학습이 불가능하다는 한계를 가진다.

#### Deep Model
 <img src="https://user-images.githubusercontent.com/68312164/105061021-5ae15380-5abc-11eb-88e9-72bbb555adab.png" width="345" height="300"> <img src="https://user-images.githubusercontent.com/68312164/105061111-79dfe580-5abc-11eb-9673-ee80dec0d90f.png" width="345" height="300">

다음으로 Deep 모델에서는 구체화된 ‘Priceline’, ‘Kayak’ 과 같은 앱이 아닌 ‘Travel’ 이라는 큰 범주 하에서 feature를 구성하게 된다. 이 경우, 위에서 0 이었던 값들이 Dense한 임베딩 공간(오른쪽)으로 옮겨져서 값을 부여받아서 더욱 일반화 된(큰 범주에서) 값을 잘 표현할 수 있게된다. 하지만, 앞서 표현했던 뚜렷한 niche 한 관계는 잘 표현하지 못한다는 단점을 가지게 된다. 이러한 장단점을 잘 표현한 그림은 아래와 같다.  

<img src="https://user-images.githubusercontent.com/68312164/105061144-849a7a80-5abc-11eb-95ca-d5e121b80c8a.png" width="345" height="300">   <img src="https://user-images.githubusercontent.com/68312164/105061198-98de7780-5abc-11eb-8d7a-5e6d6e56c9d5.png" width="345" height="300">

먼저 왼쪽은 Deep 모델이 유리한 경우인데, 상대적으로 유사한 가중치를 가질 때 임베딩 공간으로 옮겨지게 되면서 파라미터 수가 줄어드는 것을 확인할 수 있다. 이러한 특성은 추천시스템의 MF에서 SVD와 같은 latent space가 가지는 의미와 일맥상통하고 PCA 특성 또한 함유하고 있다. 반면 오른쪽에서와 같이 Spase하고 niche 할때는 Deep 모델이 상대적으로 불리할 수 있고 같은 임베딩 공간 내에서 서로 다른 클러스터를 밀어내기가 힘들어 상대적으로 덜 유의한 앱을 추천할 가능성을 가지게 된다.


즉, 정리하면
- Dense, similar -> Deep 유리
- Sparse, niche -> Wide 유리


### 2. Recommender System Overview
 <img src="https://user-images.githubusercontent.com/68312164/105073305-3be9be00-5aca-11eb-81af-4d86a8b1437e.png" width="650" height="330">

추천시스템의 개략적인 구조를 살펴보면 위와 같은데, 쿼리를 인풋으로 받아서 유저에게 Item 간 랭크를 매겨 추천해주고 이에 대한 액션과 앞선 쿼리를 로그에 담아 학습하고 다시 랭킹을 메기는 시스템 구조이다.
이 논문에서는 랭킹을 메기는 과정에서 Wide&Deep 학습 방법을 기반으로 하는 것에 중점을 둔다.

### 3. Wide & Deep Learning

#### The Wide Component
 <img src="https://user-images.githubusercontent.com/68312164/105074368-98011200-5acb-11eb-9980-307b80445dd8.png" width="650" height="330">

Wide 모델에서는 아래와 같은 선형의 관계를 나타내게 되는데, 이때 feature set $$\textbf{x}$$ 는 raw input feature와 transformed된 feature로 구성된다.

$$y = \textbf{w}^T \textbf{x} + b$$

$$where, \textbf{x} = [x_1,x_2,...,x_d],  \textbf{w} = [w_1,w_2,...,w_d],  b = bias$$

이때, raw input과 함께 구성되는 feature는 _cross-product transformation_ 을 거치게 되는데 아래와 같이 정의된다.

$$\phi_{k}(\textbf{x}) = \prod_{i=1}^{d}x_{i}^{c_{ki}}, c_{ki} \in \left \{ 0,1 \right\}$$

간단한 예시로는, 다음과 같이 binary한 상황을 가정하면 아래와 같이 표현할 수 있다.

$$gender = \left [ male, female \right ] = \left [ 1,0 \right ]$$

$$married = \left [ yes, no \right ] = \left [ 1,0 \right ]$$

$$language = \left [ english, koeran \right ] = \left [ 1,0 \right ]$$

이때, user A가 male이고 married 한 상태이고 korean 이면 아래와 같은 벡터를 가지게 된다.

$$\textbf{x} = \left [ gender, married, language \right ] = \left [ 1,1,0 \right ]$$

이때, transformation $$\phi_{k} \left ( k = 1,2,3 \right)$$ 에서 $$k=1$$인 상태가, married와 language의 조합을 나타내는 transformation이라고 한다면,
$$\phi_{1} \left (\textbf{x} \right )$$ 는 아래와 같이 나타낼 수 있다.

$$ \phi_{1} \left (\textbf{x} \right ) = x_1^{c_{11}}x_2^{c_{21}}x_3^{c_{31}} = 1^0 1^1 0^1 = 1 $$

#### The Deep Component
 <img src="https://user-images.githubusercontent.com/68312164/105074332-8f104080-5acb-11eb-9a91-07e5b6f9f0c3.png" width="650" height="330">

Deep 모델에서는 순전파의 신경망을 생각하면 되는데, 이때 $$\textbf{a}$$는 Continuous feature와 Categorical feature를 concat하여 input으로 사용한다. Categorical feature는 임베딩을 거쳐 dense한 벡터로 구성되어 합쳐진다.

$$\textbf{a}^{\left ( l+1 \right )} = f\left (\textbf{W}^{\left ( l \right )}\textbf{a}^{\left ( l \right )}+\textbf{b}^{\left ( l \right )}   \right)$$

이떼, $$l$$은 레이어 번호, $$f$$는 ReLu와 같은 활성화 함수, $$W$$는 가중치 행렬, $$b$$는 편향을 의미한다.

#### Joint Training of Wide & Deep Model
 <img src="https://user-images.githubusercontent.com/68312164/105078865-ed402200-5ad1-11eb-820d-b3919a06bb5a.png" width="650" height="330">

최종 모델은 앞선 wide, deep 모델의 특성을 합쳐서 하나의 목적식, prediction으로 표현한다고 볼 수 있다. logistic regression 문제에서 모델의 예측식은 아래와 같다.
이때, 아웃풋에서 출발하는 역전파를 wide, deep 방향 모두가 전파 받고 모델이 동시에 mini-batch 학습을 하게 된다. 동시에 두 파트가 학습되는 점이 각 모델들이 학습된 이후 최종적으로 합쳐지는 앙상블 모델과 차별화되는 점이다. 실제 Wide & Deep 실험에서는, wide 파트에서는 FTRL + L1 알고리즘을 사용하였고, deep 파트에서는 AdaGrad 를 optimizer로 사용하였다. 아래 식에서 $$P(Y=1 | \textbf{x})$$는 feature vector가 인풋으로 주어졌을 때 실제 특정 앱을 수용할 확률, app acquisition 확률로 이해하면 된다.

$$P(Y=1 | \textbf{x}) = \sigma\left ( \textbf{w}_{wide}^T\left [ \textbf{x}, \phi\left ( \textbf{x} \right ) \right ] + \textbf{w}_{deep}^T a^{\left ( l_{f} \right )} + b \right) $$


### 4. System Implementation

앱 추천 시스템의 파이프라인은 아래와 같이 Data Generation, Model Training, Model Serving 3 단계로 구성된다.  
 <img src="https://user-images.githubusercontent.com/68312164/105080633-70627780-5ad4-11eb-893b-3a35e896fb30.png" width="650" height="330">

이때, Model training 단계에서 매번 재학습을 방지하기 위해 **warm-starting**로 이전 모델의 임베딩과 linear 모델 가중치를 새로운 모델에 적용한다는 점도 주목할 만한 점이다.

### 5. Experiment Results
<img src="https://user-images.githubusercontent.com/68312164/105080975-ef57b000-5ad4-11eb-94a5-930f01039f6c.png" width="350" height="170"><img src="https://user-images.githubusercontent.com/68312164/105080997-f67ebe00-5ad4-11eb-968d-a53b261cfa44.png" width="350" height="170">

Wide 모델은 대조군으로 설정하고 Wide & Deep 모델을 실험군으로 설정했을 때, 4% 정도의 수용률 증가를 얻어냈고, online traffic일때 더욱 좋은 퍼포먼스를 보여주었다. 또한 Batch를 더욱 분할하여 진행했을 때, 속도 또한 효과적임을 확인할 수 있었다.

### 6. Conclusion
저자는 Factorization Machines(Rendle, Steffen. 2010) 에서 많은 영향을 받았다고 한다. 그리고 FM에서 주 컨셉이었던 임베딩 공간에서의 벡터 내적 대신에 신경망과 결합된 임베딩으로 한층 더 비선형적인 관계까지 확장해나갔다. 이로써 다시 한 번 정리하면, Memorization을 Wide linear model에서 cross product를 통해 이루어 냈고, Generalization을 DNN을 통해 효과적으로 이루어 내어 두 컨셉을 결합해 하나의 통합적인 모델을 제시하였다.

### 세 줄 요약
1. Memorization = Wide Linear Model + Cross-product transformation
2. Generalization = Deep Neural Network
3. Wide & Deep = Memorization + Generalization

### References
1. Cheng, Heng-Tze, et al. "Wide & deep learning for recommender systems." Proceedings of the 1st workshop on deep learning for recommender systems. 2016.
2. Wide & Deep Learning: Memorization + Generalization with TensorFlow (TensorFlow Dev Summit 2017), https://www.youtube.com/watch?v=NV1tkZ9Lq48
3. https://leehyejin91.github.io/post-wide_n_deep/
